-----------------------------
爬虫API：
·Requests-- 自动爬取HTML页面，进行网络提交
·robots.txt-网络爬虫排除标准
·Beautiful Soup库--解析html页面
·Project----实战项目A/B
·Re---------正则表达式库
·Scrapy*----专业网络爬虫框架、网络爬虫原理介绍

-----------------------------
python开发工具选择：
·文本工具类
	IDLE:python自带，默认，常用，入门工具
	Sublime Text：第三方专用编程工具（免费）
	Notepad++
·集成工具类
	PyCharm（社区版免费，简单，集成度高，适合较复杂工程）
	Anaconda&Spyder（开源免费：专门针对科学计算和数据分析的）
	Wing（收费专业：适用于公司，版本控制，版本同步，适合多人开发）
	PyDev&Eclipse（开源的）
	Visual Studio&PTVS（微软的）

-----------------------------
Requests库入门：www.python-requests.org
·安装：cmd-pip install requests
·测试：打开python命令窗口（shell）
	>>> import requests
	>>> r=requests.get("http://www.baidu.com")
	>>> r.status_code       //响应状态码返回
	200
	>>> r.encoding='utf8'	//编码重置
	>>> r.encoding		//Http header中猜测的响应内容编码方式
	'utf-8'			//如果header中不存在charset则认为编码为IOS-8859-1
	>>> r.text		//Http响应内容的字符串形式：url对应的页面内容
	>>> r.apparent_encoding	//内容中分析出的响应内容编码方式（备选编码方式）
	'utf-8'
	>>> r.content		//Http响应内容的二进制形式（例如图片）
·具体介绍：
1.Requests库的get()
	r=requests.get(url,pararms=NONE,**kwargs)//Response响应、Request请求	
2.爬去网页的通用代码框架：
	1.因为网络原因，所以网络异常处理也很重要
	2.常用的六种Requests库的异常：
		--1.requests.ConnectionError 网络连接错误异常，如DNS查询失败，拒绝连接等
		--2.requests.HTTPError	     http错误异常
		--3.requests.URLRequired     url缺失异常
		--4.requests.TooManyRedirects 超过最大重定向次数
		--5.requests.ConnectionTimeout 连接远程服务器超时异常
		--6.requests.Timeout	     请求url超时，产生超时异常
		--7.方法：r.raise_for_status():如果不是200产生异常requests.HTTPError	

	3.爬取网页代码框架：
	import requests
	def getHTMLText(url):
    	try:
        	r=requests.get(url,timeout=30)
        	r.raise_for_status()
        	r.encoding=r.apparent_encoding
        	return r.text
    	except:
        	return '产生异常'
	print(getHTMLText('http://www.baidu.com'))
	print('------------------------------------')
	payload={'key1':'va1','key2':'va2'}
	#r=requests.post('http://httpbin.org/post',data=payload)#自动编码为form表单、
	r=requests.post('http://httpbin.org/post',data='abc')#自动编码为form表单、
	print(r.text)

	#requests.request(method,url,**kwargs(其他控制访问参数))
	#method:请求方式，对应get/put/post/head/patch/delete/options等7种
	kv={'key1':'va1','key2':'va2'}
	r=requests.request('GET','http://httpbin.org/post',params=kv)
	print (r.url)
	#http://httpbin.org/post?key1=va1&key2=va2
3.Requests库的七个主要方法：
	--HTTP协议：超文本传输协议，基于请求与响应模式的无状态的应用层协议
	1.GET   请求获取URL位置的资源
	2.HEAD  请求获取URL位置资源的响应消息报告，获取头部信息
	3.POST  请求想url位置资源后附加的新的数据
	4.PUT   请求向url位置存储一个资源，覆盖原来url位置的资源
	5.PATCH 请求局部更新url位置的资源，改变该处资源的部分内容--可以节省网络带宽
	6.DELETE请求删除url位置存储的资源

-------------------------------------------------
two part：
	1.解析html页面信息标记与提取方法：Beautiful Soup
	2.实战项目：中国大学排名爬虫（实例）

Beautiful Soup库：解析，遍历，维护标签树的功能库
·安装：cmd -- pip install beautifulsoup4
·测试：
	演示地址：http://python123.io/ws/demo.html
	查看页面源代码：-----
	>>> import requests
	>>> r=requests.get('http://python123.io/ws/demo.html')
	>>> r.text
	>>> demo=r.text
	>>> from bs4 import BeautifulSoup
	>>> soup=BeautifulSoup(demo,'html.parser')
	#soup=BeautifulSoup(open("d://demo.html"),'html.parser')
	>>> print (soup)
·引用方式：
	1.from bs4 import BeautifulSoup
	2.import bs4
·基本元素：<p></p>
·BeautifulSoup库解析器：
	1.bs4的html解析器---BeautifulSoup(demo,'html.parser')--安装bs4库
	2.lxml的html解析器--BeautifulSoup(demo,'lxml')---------pip install lxml
	3.lxml的xml解析器---BeautifulSoup(demo,'xml')----------pip install lxml
	4.html5lib的解析器--BeautifulSoup(demo,'html5lib')-----pip install html5lib

















