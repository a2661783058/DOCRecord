-----------------------------
爬虫API：
·Requests-- 自动爬取HTML页面，进行网络提交
·robots.txt-网络爬虫排除标准
·Beautiful Soup库--解析html页面
·Project----实战项目A/B
·Re---------正则表达式库
·Scrapy*----专业网络爬虫框架、网络爬虫原理介绍

-----------------------------
python开发工具选择：
·文本工具类
	IDLE:python自带，默认，常用，入门工具
	Sublime Text：第三方专用编程工具（免费）
	Notepad++
·集成工具类
	PyCharm（社区版免费，简单，集成度高，适合较复杂工程）
	Anaconda&Spyder（开源免费：专门针对科学计算和数据分析的）
	Wing（收费专业：适用于公司，版本控制，版本同步，适合多人开发）
	PyDev&Eclipse（开源的）
	Visual Studio&PTVS（微软的）

-----------------------------
Requests库入门：www.python-requests.org
·安装：cmd-pip install requests
·测试：打开python命令窗口（shell）
	>>> import requests
	>>> r=requests.get("http://www.baidu.com")
	>>> r.status_code       //响应状态码返回
	200
	>>> r.encoding='utf8'	//编码重置
	>>> r.encoding		//Http header中猜测的响应内容编码方式
	'utf-8'			//如果header中不存在charset则认为编码为IOS-8859-1
	>>> r.text		//Http响应内容的字符串形式：url对应的页面内容
	>>> r.apparent_encoding	//内容中分析出的响应内容编码方式（备选编码方式）
	'utf-8'
	>>> r.content		//Http响应内容的二进制形式（例如图片）
·具体介绍：
1.Requests库的get()
	r=requests.get(url,pararm=NONE,**kwargs)//Response响应、Request请求	
2.爬去网页的通用代码框架：
	1.因为网络原因，所以网络异常处理也很重要
	2.常用的六种Requests库的异常：
		--1.requests.ConnectionError 网络连接错误异常，如DNS查询失败，拒绝连接等
		--2.requests.HTTPError	     http错误异常
		--3.requests.URLRequired     url缺失异常
		--4.requests.TooManyRedirects 超过最大重定向次数
		--5.requests.ConnectionTimeout 连接远程服务器超时异常
		--6.requests.Timeout	     请求url超时，产生超时异常
		--7.方法：r.raise_for_status():如果不是200产生异常requests.HTTPError	

	3.爬取网页代码框架：
	import requests
	def getHTMLText(url):
    	try:
        	r=requests.get(url,timeout=30)
        	r.raise_for_status()
        	r.encoding=r.apparent_encoding
        	return r.text
    	except:
        	return '产生异常'
	print(getHTMLText('http://www.baidu.com'))
3.Requests库的七个主要方法：
	--HTTP协议：超文本传输协议，基于请求与响应模式的无状态的应用层协议
	1.GET   请求获取URL位置的资源
	2.HEAD  请求获取URL位置资源的响应消息报告，获取头部信息
	3.POST  请求想url位置资源后附加的新的数据
	4.PUT   请求向url位置存储一个资源，覆盖原来url位置的资源
	5.PATCH 请求局部更新url位置的资源，改变该处资源的部分内容--可以节省网络带宽
	6.DELETE请求删除url位置存储的资源




















